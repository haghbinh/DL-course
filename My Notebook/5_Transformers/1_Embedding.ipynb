{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding in NLP\n",
    "\n",
    "Understanding the concept of embeddings in natural language processing with an example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Terms:\n",
    "1. **Vocabulary ($\\mathcal{V}$)**: A set of all possible unique tokens (words, symbols, etc.) in the dataset. For example, if we have a vocabulary of 30,000 words, $|\\mathcal{V}| = 30,000$.\n",
    "\n",
    "2. **Embedding Size ($D$)**: The size of the vector that represents each token. For example, if each word is represented as a 1024-dimensional vector, $D = 1024$.\n",
    "\n",
    "3. **Embedding Matrix ($\\Omega_e$)**: A matrix of size $D \\times |\\mathcal{V}|$. Each column in this matrix represents the vector (embedding) of a token in the vocabulary. This matrix is learned during training.\n",
    "\n",
    "4. **One-Hot Encoding**: A way to represent tokens as vectors. For a vocabulary of size $|\\mathcal{V}|$, each token is represented as a $|\\mathcal{V}|$-dimensional vector where all values are 0 except for one position, which is set to 1. For example:\n",
    "   - \"cat\" → [0, 1, 0, 0, ...]\n",
    "   - \"dog\" → [0, 0, 1, 0, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Process:\n",
    "1. **Example Vocabulary**:\n",
    "   Let our vocabulary be:\n",
    "   $$ \n",
    "   \\mathcal{V} = \\{ \\text{\"The\", \"cat\", \"sat\", \"on\", \"mat\"} \\}\n",
    "   $$ \n",
    "   Thus, $|\\mathcal{V}| = 5$.\n",
    "\n",
    "2. **Input Tokens**:\n",
    "   Suppose our input sequence is:\n",
    "   $$ \n",
    "   \\text{Tokens} = [\\text{\"The\"}, \\text{\"cat\"}, \\text{\"sat\"}] \n",
    "   $$ \n",
    "   There are $N = 3$ tokens in this sequence.\n",
    "\n",
    "3. **One-Hot Matrix ($T$)**:\n",
    "   Convert these tokens into a one-hot encoded matrix $T$ of size $|\\mathcal{V}| \\times N$:\n",
    "   $$ \n",
    "   T = \\begin{bmatrix} \n",
    "        1 & 0 & 0 \\\\\n",
    "        0 & 1 & 0 \\\\\n",
    "        0 & 0 & 1 \\\\\n",
    "        0 & 0 & 0 \\\\\n",
    "        0 & 0 & 0\n",
    "   \\end{bmatrix} \n",
    "   $$ \n",
    "   Here, each column corresponds to one token, and the rows correspond to vocabulary terms.\n",
    "\n",
    "4. **Embedding Matrix ($\\Omega_e$)**:\n",
    "   Suppose $D = 3$ (embedding size). The embedding matrix $\\Omega_e$ will have random initial values:\n",
    "   $$ \n",
    "   \\Omega_e = \\begin{bmatrix}\n",
    "       0.1 & 0.4 & 0.3 & 0.2 & 0.5 \\\\\n",
    "       0.6 & 0.1 & 0.2 & 0.7 & 0.3 \\\\\n",
    "       0.8 & 0.9 & 0.4 & 0.5 & 0.6\n",
    "   \\end{bmatrix}\n",
    "   $$ \n",
    "   Each column of $\\Omega_e$ corresponds to the embedding for each token in the vocabulary.\n",
    "\n",
    "5. **Input Embeddings ($X$)**:\n",
    "   Multiply the one-hot encoded matrix $T$ with the embedding matrix $\\Omega_e$ to compute the input embeddings $X$. This is the transformation:\n",
    "   $$ \n",
    "   X = \\Omega_e T.\n",
    "   $$ \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **torch.nn** module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix (Omega_e):\n",
      "tensor([[-0.8805, -0.6517,  0.4077],\n",
      "        [ 0.4389, -1.1243, -0.8373],\n",
      "        [ 1.3981, -1.4097,  0.8434],\n",
      "        [ 2.0104,  2.2844,  0.1933],\n",
      "        [ 0.7380,  0.5161,  1.5216]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_size = 5  # Number of unique tokens\n",
    "embedding_dim = 3  # Size of each embedding vector\n",
    "\n",
    "# Create an embedding layer\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "# Print results\n",
    "print(\"Embedding matrix (Omega_e):\")\n",
    "print(embedding_layer.weight.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example token indices\n",
    "token_indices = torch.tensor([0, 1, 3, 4])  # \"cat\", \"dog\", \"bird\", \"fish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded matrix:\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "Embeddings for tokens:\n",
      "tensor([[-0.8805, -0.6517,  0.4077],\n",
      "        [ 0.4389, -1.1243, -0.8373],\n",
      "        [ 2.0104,  2.2844,  0.1933],\n",
      "        [ 0.7380,  0.5161,  1.5216]])\n"
     ]
    }
   ],
   "source": [
    "# Create the one-hot encoded matrix\n",
    "T = torch.nn.functional.one_hot(token_indices, num_classes=vocab_size).to(torch.float)\n",
    "print(\"One-hot encoded matrix:\")\n",
    "print(T)\n",
    "print(\"Embeddings for tokens:\")\n",
    "print(T@embedding_layer.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for tokens:\n",
      "tensor([[-0.8805, -0.6517,  0.4077],\n",
      "        [ 0.4389, -1.1243, -0.8373],\n",
      "        [ 2.0104,  2.2844,  0.1933],\n",
      "        [ 0.7380,  0.5161,  1.5216]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get embeddings using embedding_layer\n",
    "X = embedding_layer(token_indices)\n",
    "print(\"Embeddings for tokens:\")\n",
    "print(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
