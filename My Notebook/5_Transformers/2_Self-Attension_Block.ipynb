{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention Block in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Self-Attention block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        # Learnable weight matrices for Q, K, and V\n",
    "        self.query = nn.Linear(input_dim, output_dim)\n",
    "        self.key = nn.Linear(input_dim, output_dim)\n",
    "        self.value = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([output_dim]))  # Scaling factor\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Step 1: Compute Q, K, V\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        # Step 2: Compute the attention scores\n",
    "        attention_scores = Q @ K.transpose(-2, -1) / self.scale\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        output = attention_weights@V\n",
    "        \n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .transpose(-2, -1) in K.transpose(-2, -1) is a way to specify that you want to transpose the last two dimensions of a tensor.\n",
    "Letâ€™s break it down:\n",
    "* -1 refers to the last dimension (e.g., columns in a 2D matrix or features in a 3D tensor).\n",
    "* -2 refers to the second-to-last dimension (e.g., rows in a 2D matrix or sequence length in a 3D tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output:\n",
      "tensor([[-0.3173, -0.0280,  0.0224],\n",
      "        [-0.4334, -0.1403,  0.1004],\n",
      "        [-0.3490, -0.0794,  0.0435],\n",
      "        [-0.2562,  0.0009, -0.0200]], grad_fn=<MmBackward0>)\n",
      "Attention Weights:\n",
      "tensor([[0.2688, 0.2628, 0.2259, 0.2425],\n",
      "        [0.2219, 0.2686, 0.2560, 0.2536],\n",
      "        [0.2636, 0.2650, 0.2494, 0.2220],\n",
      "        [0.2799, 0.2791, 0.2216, 0.2194]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 3\n",
    "\n",
    "torch.manual_seed(10)\n",
    "self_attention = SelfAttention(input_dim=embedding_dim, output_dim=3)\n",
    "\n",
    "X = torch.tensor([[-0.8805, -0.6517,  0.4077],\n",
    "        [ 0.4389, -1.1243, -0.8373],\n",
    "        [ 2.0104,  2.2844,  0.1933],\n",
    "        [ 0.7380,  0.5161,  1.5216]], dtype=torch.float32)  # Shape: D=3, N=5\n",
    "\n",
    "# Forward pass through the self-attention block\n",
    "output, attention_weights = self_attention(X)\n",
    "\n",
    "print(\"Attention Output:\")\n",
    "print(output)\n",
    "print(\"Attention Weights:\")\n",
    "print(attention_weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
